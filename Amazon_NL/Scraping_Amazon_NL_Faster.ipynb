{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de48cf6a-e59b-42a5-b52b-a656bf402311",
   "metadata": {},
   "source": [
    "# At First Scraping The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6d60b-45f6-4d23-839a-e4db87005167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "category_name_list = []\n",
    "all_phones_pages_link_list = []\n",
    "all_phones_link_list = []\n",
    "all_headphones_pages_link_list = []\n",
    "all_headphones_link_list = []\n",
    "all_covers_pages_link_list = []\n",
    "all_covers_link_list = []\n",
    "all_screens_pages_link_list = []\n",
    "all_screens_link_list = []\n",
    "all_power_banks_pages_link_list = []\n",
    "all_power_banks_link_list = []\n",
    "all_chargers_pages_link_list = []\n",
    "all_chargers_link_list = []\n",
    "headers = {\n",
    "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\",\n",
    "\"Accept-Encoding\": \"gzip, deflate\",\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8\",\n",
    "\"DNT\": \"1\",\n",
    "\"Connection\": \"close\",\n",
    "\"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "def scrape_proxies(url, limit=101):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all table rows\n",
    "        rows = soup.find_all('tr')\n",
    "        \n",
    "        # Extract IP and Port from each row and format as requested\n",
    "        proxies = [\n",
    "            {'http': f\"{row.find_all('td')[0].text}:{row.find_all('td')[1].text}\"}\n",
    "            for row in rows[:limit]\n",
    "            if len(row.find_all('td')) >= 2\n",
    "        ]\n",
    "        \n",
    "        return proxies\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "mohamed = \"https://free-proxy-list.net\"  # Replace with the actual mohamed\n",
    "proxies = scrape_proxies(mohamed)\n",
    "\n",
    "for i in range(1,34):\n",
    "    product_link = f'https://www.amazon.nl/s?i=electronics&rh=n%3A16366059031&s=popularity-rank&fs=true&page={i}&language=en_GB'\n",
    "    all_phones_pages_link_list.append(product_link)\n",
    "proxy_failures = defaultdict(int)\n",
    "MAX_FAILURES = 101\n",
    "def get_proxy():\n",
    "    working_proxies = [p for p, failures in proxy_failures.items() if failures < MAX_FAILURES]\n",
    "    return random.choice(working_proxies) if working_proxies else None\n",
    "\n",
    "\n",
    "def get_text_or_none(tag, attribute_name=None):\n",
    "    if tag:\n",
    "        if attribute_name:\n",
    "            return tag.get(attribute_name, None)\n",
    "        return tag.get_text(strip=True)\n",
    "    return None\n",
    "def make_request(url, max_retries=51):        \n",
    "    for attempt in range(max_retries):\n",
    "        proxy = get_proxy()\n",
    "        if not proxy:\n",
    "            # print(\"No working proxies available. Trying without proxy.\")\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                # print(f\"Attempt {attempt + 1} failed without proxy: {e}\")\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, proxies={'http': proxy})\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            # print(f\"Attempt {attempt + 1} failed with proxy {proxy}: {e}\")\n",
    "            proxy_failures[proxy] += 1\n",
    "            if proxy_failures[proxy] >= MAX_FAILURES:\n",
    "                # print(f\"Proxy {proxy} has failed {MAX_FAILURES} times and will no longer be used.\")\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "    \n",
    "    # raise Exception(f\"Failed to retrieve {url} after {max_retries} attempts\")\n",
    "\n",
    "def get_all_phones_link(link, max_retries=35):\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            page = make_request(link)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            all_phones = soup.find_all(\"div\", class_=\"a-section a-spacing-none a-spacing-top-small s-title-instructions-style\")\n",
    "            \n",
    "            if all_phones:\n",
    "                for phone in all_phones:\n",
    "                    phone_link = phone.find_all(\"a\")[0].get(\"href\")\n",
    "                    phone_link = f\"https://www.amazon.nl{phone_link}\"\n",
    "                    all_phones_link_list.append(phone_link)\n",
    "                \n",
    "                print(f\"Found {len(all_phones)} phones on page {link}\")\n",
    "                return\n",
    "            \n",
    "            else:\n",
    "                retry_count += 1\n",
    "                # print(f\"No phones found on page {link}, retrying... ({retry_count}/{max_retries})\")\n",
    "                time.sleep(random.uniform(1, 2))  # Random delay before retrying\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error getting phone links from {link}: {e}\")\n",
    "            retry_count += 1\n",
    "            time.sleep(random.uniform(1, 2))  # Random delay before retrying\n",
    "    \n",
    "    # print(f\"Failed to find phones on {link} after {max_retries} retries\")\n",
    "\n",
    "\n",
    "# Modified scrape_product_data function\n",
    "def scrape_product_data(link, max_retries=500):\n",
    "    for attempt in range(max_retries):\n",
    "        # time.sleep(random.uniform(1, 2))\n",
    "        try:\n",
    "            page = make_request(link)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        \n",
    "            camera_info = None\n",
    "            for li in soup.find_all('li', class_='a-spacing-mini'):\n",
    "                if 'MP' in li.text:\n",
    "                    camera_info = re.search(r'(\\d+MP)', li.text)\n",
    "                    if camera_info:\n",
    "                        camera_info = camera_info.group(1)\n",
    "                        break\n",
    "    \n",
    "            color_row = soup.find('tr', class_='a-spacing-small po-color')\n",
    "            color = None  # Initialize color\n",
    "            if color_row:\n",
    "                # If the color row exists, find the span inside it and get the text\n",
    "                color = get_text_or_none(color_row.find('span', class_='a-size-base po-break-word'))\n",
    "            \n",
    "            elif soup.find('table', id ='productDetails_techSpec_section_1'):\n",
    "                color_row2 = soup.find('th', string='Color')\n",
    "                if color_row2:\n",
    "                    color = color_row2.find_next('td').get_text(strip=True)\n",
    "            \n",
    "            else:\n",
    "                # Check for color in div with id='variation_color_name'\n",
    "                outerSelect = soup.find('div', id='variation_color_name')\n",
    "                if outerSelect:\n",
    "                    selection_row = outerSelect.find('span', class_='selection')\n",
    "                    if selection_row:\n",
    "                        color = get_text_or_none(selection_row)\n",
    "    \n",
    "            price = get_text_or_none(soup.find('div', class_='a-section a-spacing-micro').find('span', class_='a-offscreen')\n",
    "                        if soup.find('div', class_='a-section a-spacing-micro') else None)\n",
    "            if not price:\n",
    "                price = get_text_or_none(soup.find('span', class_='a-price-whole'))\n",
    "    \n",
    "            Website = 'Amazon NL'\n",
    "            Currency = 'EUR'\n",
    "    \n",
    "            # Extract other product data\n",
    "            data = {\n",
    "                # \"category\": category_name_list[0] if category_name_list else \"unknown-category\",\n",
    "                \"Title\": get_text_or_none(soup.find('span', id='productTitle')),\n",
    "                \"Brand\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-brand').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-brand') else None),\n",
    "                \"OS\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-operating_system').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-operating_system') else None),\n",
    "                \"RAM\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-ram_memory.installed_size').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-ram_memory.installed_size') else None),\n",
    "                \"CPU\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-cpu_model.family').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-cpu_model.family') else None),\n",
    "                \"Storage\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-memory_storage_capacity').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-memory_storage_capacity') else None),\n",
    "                \"Screen Size\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-display.size').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-display.size') else None),\n",
    "                \"Resolution\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-resolution').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-resolution') else None),\n",
    "                \"CPU Speed\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-cpu_model.speed').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-cpu_model.speed') else None),\n",
    "                \"Model\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-model_name').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-model_name') else None),\n",
    "                \"Wireless Provider\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-wireless_provider').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-wireless_provider') else None),\n",
    "                \"Cellular Technology\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-cellular_technology').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-cellular_technology') else None),\n",
    "                \"Color\": color,\n",
    "                \"Refresh Rate\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-refresh_rate').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-refresh_rate') else None),\n",
    "                \"SIM Count\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-sim_card_slot_count').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-sim_card_slot_count') else None),\n",
    "                \"Wireless Technology\": get_text_or_none(soup.find('tr', class_='a-spacing-small po-wireless_network_technology').find('span', class_='a-size-base po-break-word') if soup.find('tr', class_='a-spacing-small po-wireless_network_technology') else None),\n",
    "                \"Price\": price,\n",
    "                \"Price Before Promotion\": get_text_or_none(soup.find('span', class_='a-price a-text-price').find('span', class_='a-offscreen') if soup.find('span', class_='a-price a-text-price') else None),\n",
    "                \"Rate\": get_text_or_none(soup.find('span', class_='a-icon-alt')),\n",
    "                \"Camera\": camera_info,\n",
    "                \"IMG\": get_text_or_none(soup.find('img', class_='a-dynamic-image'), 'data-old-hires') or get_text_or_none(soup.find('img', class_='a-dynamic-image'), 'src'),\n",
    "                \"Website\" : Website,\n",
    "                'Currancy' : Currency,\n",
    "                \"URL\" : link\n",
    "            }\n",
    "    \n",
    "            # Extract reviews\n",
    "            reviews_from_egypt = []\n",
    "            reviews_outside_egypt = []\n",
    "    \n",
    "            review_elements = soup.find_all(\"div\", class_=\"a-section review aok-relative\")\n",
    "            for review in review_elements:\n",
    "                reviewer_name = get_text_or_none(review.find(\"span\", class_=\"a-profile-name\"))\n",
    "                rating = get_text_or_none(review.find(\"span\", class_=\"a-icon-alt\"))\n",
    "                review_date = get_text_or_none(review.find(\"span\", class_=\"a-size-base a-color-secondary review-date\"))\n",
    "                review_text = get_text_or_none(review.find(\"span\", class_=\"a-size-base review-text review-text-content\"))\n",
    "                \n",
    "                review_info = f\"Name: {reviewer_name}, Rating: {rating}, Date: {review_date}, Review: {review_text}\"\n",
    "                \n",
    "                if \"Verified Purchase\" in review.text:\n",
    "                    reviews_from_egypt.append(review_info)\n",
    "                else:\n",
    "                    reviews_outside_egypt.append(review_info)\n",
    "    \n",
    "            data[\"Reviews\"] = reviews_from_egypt\n",
    "            if data['Title']:\n",
    "                print(f\"Successfully scraped data for {data['Title']}\")\n",
    "                return data\n",
    "            else:\n",
    "                continue\n",
    "                # print(f\"No title found for {link}, retrying... (Attempt {attempt + 1}/{max_retries})\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            # print(f\"Error scraping {link}: {e}. Retrying... (Attempt {attempt + 1}/{max_retries})\")\n",
    "    \n",
    "    print(f\"Failed to scrape {link} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Main execution\n",
    "    # Get all phone links with retries on individual pages\n",
    "for proxy in proxies:\n",
    "    proxy_failures[proxy['http']] = 0\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(get_all_phones_link, all_phones_pages_link_list)\n",
    "\n",
    "print(f\"Total phone links found: {len(all_phones_link_list)}\")\n",
    "\n",
    "if len(all_phones_link_list) == 0:\n",
    "    print(\"No phone links found. Exiting...\")\n",
    "else:\n",
    "    # Scrape product data\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        future_to_url = {executor.submit(scrape_product_data, url): url for url in all_phones_link_list}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:\n",
    "                    results.append(data)\n",
    "            except Exception as exc:\n",
    "                print(f'{url} generated an exception: {exc}')\n",
    "    \n",
    "    print(f\"Successfully scraped data for {len(results)} phones\")\n",
    "    \n",
    "    df1 = pd.DataFrame(results)\n",
    "    \n",
    "    if not df1.empty:\n",
    "        print(\"Data scraping and saving completed.\")\n",
    "    else:\n",
    "        print(\"No data was scraped. Check the logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5da285-e9b7-4b9a-a61b-3a804146b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27025192-747c-4cbb-b901-a4909558a961",
   "metadata": {},
   "source": [
    "# Then Cleaning The Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c61e3-31f8-4b58-a351-13a91187162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import ast\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "null = None\n",
    "df=df1.copy()\n",
    "\n",
    "def scrape_the_price(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "    }\n",
    "    PageToScrape = requests.get(url, headers=headers)\n",
    "    if PageToScrape.status_code == 200:\n",
    "        soup = BeautifulSoup(PageToScrape.text, 'html.parser')\n",
    "        convert_tag = soup.find('div', class_='YMlKec fxKbKc')\n",
    "        convert = convert_tag.get_text(strip = True)\n",
    "        return float(convert)\n",
    "SAR_USD = scrape_the_price(\"https://www.google.com/finance/quote/SAR-USD\")\n",
    "EUR_USD = scrape_the_price(\"https://www.google.com/finance/quote/EUR-USD\")\n",
    "EGP_USD = scrape_the_price(\"https://www.google.com/finance/quote/EGP-USD\")\n",
    "\n",
    "\n",
    "\n",
    "def convert_price_before(value):\n",
    "    value =str(value)\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            value = value.upper()\n",
    "            value = value.replace(',', '')\n",
    "            value = value.replace('SAR','')\n",
    "            value = value.replace('EUR','')\n",
    "            value = value.replace('â‚¬','')\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "def convert_rate(value):\n",
    "    value =str(value)\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            value = value.lower()\n",
    "            value = value.replace(',', '')\n",
    "            value = value.replace('out of 5 stars','')\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "def convert_to_gb(value):\n",
    "    value = str(value)\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(',', '')\n",
    "        value = value.upper()\n",
    "        if 'GB' in value:\n",
    "            try:\n",
    "                return float(value.replace('GB', ''))\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        elif 'MB' in value:\n",
    "            try:\n",
    "                return float(value.replace('MB', '')) / 1024 \n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        elif 'TB' in value:\n",
    "            try:\n",
    "                return float(value.replace('TB', '')) * 1024 \n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        elif 'LESS THAN' in value:\n",
    "            return np.nan \n",
    "        else:\n",
    "            return np.nan \n",
    "    elif isinstance(value, (int, float)):\n",
    "        return float(value) \n",
    "    else:\n",
    "        return np.nan\n",
    "def convert_screen_size(value):\n",
    "    value = str(value)\n",
    "    value = value.upper()\n",
    "    if 'INCHES' in value:\n",
    "        value = value.replace('INCHES','')\n",
    "        return float(value)\n",
    "    elif 'INCH' in value:\n",
    "        value = value.replace('INCH','')\n",
    "        return float(value)\n",
    "    else:\n",
    "        return None\n",
    "def convert_resolution(value):\n",
    "    value = str(value)\n",
    "    if value and isinstance(value, str):\n",
    "        try:\n",
    "            width, height = value.lower().split('x')\n",
    "            width = float(width)\n",
    "            height = float(height)\n",
    "            return width, height\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "def change_price_to_USD(value,convert):\n",
    "    if value :\n",
    "        value = value*convert\n",
    "        return value\n",
    "    else:\n",
    "        return None\n",
    "def change_to_USD(value):\n",
    "    return 'USD'\n",
    "def find_cpu_in_title(title):\n",
    "    cpu_sizes=['Octa Core', 'Hexa Core', 'Quad Core', 'Deca Core']\n",
    "    if title:\n",
    "        title_lower = title.lower()\n",
    "        if 'cpu' in title_lower  :\n",
    "            for cpu in cpu_sizes:\n",
    "                if re.search(r'\\b' + re.escape(cpu.lower()) + r'\\b', title_lower, re.IGNORECASE):\n",
    "                    return cpu\n",
    "    return None\n",
    "def find_color_in_title(title):\n",
    "    allcolors = [\n",
    "    \"Rose Gold\", \"Navy Blue\",\"Light Blue\",\"Cobalt Violet\",\"Awesome Graphite\",\"Titanium Black\",\"Wave Green\",\"Emerald Green\",\"Titanium Silver\",\n",
    "    \"Midnight Blue\", \"Forest Green\", \"Pink\", \"Purple\", \"Lavender\",\"Charcoal Ink\",\"Carbon Grey\",\"Glamorous Green\",\"Flowing Silver\",\n",
    "    \"Orange\", \"Bronze\", \"Copper\", \"Champagne\", \"Beige\", \"Coral\", \"Midnight Black\",\"Cobalt Violet\",\n",
    "    \"Turquoise\", \"Teal\", \"Mint Green\", \"Burgundy\", \"Aqua\", \"Cyan\", \"Lime Green\",\"Rose Red\",\n",
    "    \"Space Gray\", \"Graphite\", \"Alpine Green\", \"Starlight\", \"Pacific Blue\", \"Ceramic White\",\"Moonlight White\"\n",
    "    \"Jet Black\", \"Matte Black\", \"Phantom Black\", \"Phantom Silver\", \"Mystic Bronze\", \"Awesome Navy\"\n",
    "    \"Mystic Green\", \"Cosmic Gray\", \"Cosmic Black\", \"Pearl White\", \"Glossy White\", \"Awesome Lime\",\"Silver Shadow\",\n",
    "    \"Aura Glow\", \"Aurora Blue\", \"Prism White\", \"Cloud Pink\", \"Midnight Green\", \"Mint\",\"Charcoal\",\n",
    "    \"Sunset Gold\", \"Ocean Blue\", \"Thunder Purple\", \"Starry Night\", \"Twilight\", \"Clear\",\"Navy\",\"Chartreuse\",\n",
    "    \"Gradient Purple\", \"Gradient Blue\", \"Gradient Red\",\"Gradient Pink\", \"Marble White\", \"Graphite\",\"Violet\",\n",
    "    \"Opal White\", \"Crystal Blue\", \"Ice Blue\", \"Sunrise Red\", \"Fiery Red\",\"Blue\",\"Black\", \"White\", \"Gray\", \"Silver\", \"Gold\", \"Red\",\"Yellow\", \"Green\", \"Brown\"\n",
    "]\n",
    "    for color in allcolors:\n",
    "        if color.lower() in title.lower():\n",
    "            return color\n",
    "    return None\n",
    "def find_strg_in_title(title):\n",
    "    storage_sizes =[\"1TB\",\"512GB\", \"256GB\", \"128GB\", \"64GB\", \"32GB\", \"16GB\", \"8GB\"]\n",
    "    if title:\n",
    "        title_lower = title.lower()\n",
    "        # Check for keywords related to RAM\n",
    "        \n",
    "        if 'storage' in title_lower  :\n",
    "            for storage in storage_sizes:\n",
    "                if re.search(r'\\b' + re.escape(storage.lower()) + r'\\b', title_lower, re.IGNORECASE):\n",
    "                    return storage\n",
    "    return None\n",
    "def find_ram_in_title(title):\n",
    "    ram_sizes = [\"1GB\",\"2GB\", \"3GB\",\"4GB\",\"6GB\",\"8GB\",\"12GB\",\"16GB\"]\n",
    "    if title:\n",
    "        title_lower = title.lower()\n",
    "        # Check for keywords related to RAM\n",
    "        if 'ram' in title_lower or 'memory' in title_lower:\n",
    "            for ram in ram_sizes:\n",
    "                if re.search(r'\\b' + re.escape(ram.lower()) + r'\\b', title_lower, re.IGNORECASE):\n",
    "                    return ram\n",
    "    return None\n",
    "df_nl=df\n",
    "df_nl.columns = [col.lower() for col in df_nl.columns]\n",
    "df_nl.rename(columns={'main camera': 'camera', 'network': 'cellular technology','number of sim':'sim count',}, inplace=True)\n",
    "df_nl['CPU'] = df_nl.apply(lambda row: find_cpu_in_title(row['title']) or row['cpu'], axis=1)\n",
    "df_nl['color'] = df_nl.apply(lambda row: find_color_in_title(row['title']) or row['color'], axis=1)\n",
    "df_nl['Storage'] = df_nl.apply(lambda row: find_strg_in_title(row['title']) or row['storage'], axis=1)\n",
    "df_nl['RAM'] = df_nl.apply(lambda row: find_ram_in_title(row['title']) or row['ram'], axis=1)\n",
    "df_nl['ram'] = df_nl['ram'].fillna(df_nl['RAM'])\n",
    "df_nl['storage'] = df_nl['storage'].fillna(df_nl['Storage'])\n",
    "df_nl['cpu'] = df_nl['cpu'].fillna(df_nl['CPU'])\n",
    "df_nl=df_nl.drop(['Storage','CPU','RAM'], axis=1)\n",
    "df_nl['storage'] = df_nl['storage'].apply(convert_to_gb)\n",
    "df_nl['ram'] = df_nl['ram'].apply(convert_to_gb)\n",
    "df_nl['screen size'] = df_nl['screen size'].apply(convert_screen_size)\n",
    "df_nl[['width resolution', 'height resolution']] = df_nl['resolution'].apply(convert_resolution).apply(pd.Series)\n",
    "df_nl['price'] = df_nl['price'].apply(convert_price_before).apply(lambda x: change_price_to_USD(x, EUR_USD))\n",
    "df_nl['price before promotion'] = df_nl['price before promotion'].apply(convert_price_before).apply(lambda x: change_price_to_USD(x, EUR_USD))\n",
    "df_nl['rate'] = df_nl['rate'].apply(convert_rate)\n",
    "df_nl['currancy'] = df_nl['currancy'].apply(change_to_USD)\n",
    "df_nl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
